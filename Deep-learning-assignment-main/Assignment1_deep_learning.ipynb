{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUgYFYI-1KcM",
        "outputId": "ef17562f-843a-466e-f463-daed19ac7a03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training AND Gate ---\n",
            "Final Weights: [0.2 0.1]\n",
            "Final Bias: -0.20000000000000004\n",
            "Predictions: [0 0 0 1]\n",
            " AND Gate learned successfully.\n",
            "\n",
            "--- Training OR Gate ---\n",
            "Final Weights: [0.1 0.1]\n",
            "Final Bias: -0.1\n",
            "Predictions: [0 1 1 1]\n",
            " OR Gate learned successfully.\n",
            "\n",
            "--- Training NAND Gate ---\n",
            "Final Weights: [-0.2 -0.1]\n",
            "Final Bias: 0.2\n",
            "Predictions: [1 1 1 0]\n",
            " NAND Gate learned successfully.\n",
            "\n",
            "--- Training NOR Gate ---\n",
            "Final Weights: [-0.1 -0.1]\n",
            "Final Bias: 0.0\n",
            "Predictions: [1 0 0 0]\n",
            " NOR Gate learned successfully.\n",
            "\n",
            "--- Training XOR Gate ---\n",
            "Final Weights: [-0.1  0. ]\n",
            "Final Bias: 0.0\n",
            "Predictions: [1 1 0 0]\n",
            " XOR Gate failed to converge (linearly inseparable).\n"
          ]
        }
      ],
      "source": [
        "#The Perceptron\n",
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, epochs=10):\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def activation(self, z):\n",
        "        \"\"\"Step function: Returns 1 if z >= 0, else 0.\"\"\"\n",
        "        return 1 if z >= 0 else 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Trains the perceptron on the given dataset.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize weights and bias to small random numbers or zeros\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            errors = 0\n",
        "            for i in range(n_samples):\n",
        "                # 1. Calculate Weighted Sum (Evidence Aggregation)\n",
        "                linear_output = np.dot(X[i], self.weights) + self.bias\n",
        "\n",
        "                # 2. Make Prediction (Decision Rule)\n",
        "                y_predicted = self.activation(linear_output)\n",
        "\n",
        "                # 3. Calculate Error Signal\n",
        "                error = y[i] - y_predicted\n",
        "\n",
        "                # 4. Update Weights and Bias if there is an error\n",
        "                if error != 0:\n",
        "                    self.weights += self.lr * error * X[i]\n",
        "                    self.bias += self.lr * error\n",
        "                    errors += 1\n",
        "\n",
        "            #  Print progress\n",
        "            # print(f\"Epoch {epoch+1}: Errors = {errors}\")\n",
        "\n",
        "            # Convergence check: If no errors, stop early\n",
        "            if errors == 0:\n",
        "                break\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predicts output for new inputs.\"\"\"\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return np.array([self.activation(z) for z in linear_output])\n",
        "\n",
        "# --- Dataset Definitions ---\n",
        "# Inputs (x1, x2)\n",
        "X = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Outputs for different gates\n",
        "gates = {# now i use and or nand nor xor\n",
        "    \"AND\": np.array([0, 0, 0, 1]),\n",
        "    \"OR\":  np.array([0, 1, 1, 1]),\n",
        "    \"NAND\": np.array([1, 1, 1, 0]),\n",
        "    \"NOR\":  np.array([1, 0, 0, 0]),\n",
        "    \"XOR\":  np.array([0, 1, 1, 0])\n",
        "}\n",
        "\n",
        "#  Training and Testing\n",
        "for gate_name, y in gates.items():\n",
        "    print(f\"\\n--- Training {gate_name} Gate ---\")\n",
        "    p = Perceptron(learning_rate=0.1, epochs=20)\n",
        "    p.fit(X, y)\n",
        "\n",
        "    predictions = p.predict(X)\n",
        "    print(f\"Final Weights: {p.weights}\")\n",
        "    print(f\"Final Bias: {p.bias}\")\n",
        "    print(f\"Predictions: {predictions}\")\n",
        "\n",
        "    # Verification\n",
        "    if np.array_equal(predictions, y):\n",
        "        print(f\" {gate_name} Gate learned successfully.\")\n",
        "    else:\n",
        "        print(f\" {gate_name} Gate failed to converge (linearly inseparable).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. Why did the XOR gate fail to converge?\n",
        "**Answer:** A single-layer perceptron can only learn functions that are **linearly separable**. This means the decision boundary must be a straight line. If you plot the inputs for XOR, you cannot draw a single straight line to separate the outputs of 0 from the outputs of 1.\n",
        "\n",
        "### Q2. What is the effect of the learning rate ?\n",
        "**Answer:**\n",
        "* **Too high:** The model oscillates and fails to settle on the correct weights, causing instability.\n",
        "* **Too low:** The model converges very slowly, requiring too many epochs.\n",
        "* A moderate value (e.g., 0.1` or 0.01) provides a good balance between speed and stability for this linear problem.\n",
        "\n",
        "### Q3. Why did the same code learn different gates?\n",
        "**Answer:** The underlying algorithm (the learning rule) remains unchanged because it defines *how* to update weights based on error. The different gates are learned because the **input-output data (the dataset)** defines the decision boundary. The weights adjust based specifically on the data presented to them.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "hEgtnkUb42Ee"
      }
    }
  ]
}