{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTEHhPKzeN3fA1iOChRK67",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bansalhim/Deep-learning-assignment/blob/main/Assignment_5_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lab 5: Linear Regression Neuron: Learning by Gradient\n",
        "Descent (No ML Libraries)**"
      ],
      "metadata": {
        "id": "7C7fv0s9NM5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Part A: Data Setup\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# A1. Load dataset\n",
        "zip_path = \"/content/abalone.zip\"   #  data is in form of zip file\n",
        "columns = [\n",
        "    'Sex', 'Length', 'Diameter', 'Height',\n",
        "    'WholeWeight', 'ShuckedWeight',\n",
        "    'VisceraWeight', 'ShellWeight', 'Rings'\n",
        "]\n",
        "\n",
        "# Open the zip file and read the 'abalone.data' file\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    with z.open('abalone.data') as f:\n",
        "        data = pd.read_csv(io.TextIOWrapper(f, 'utf-8'), names=columns)\n",
        "\n",
        "# Print dataset info\n",
        "print(\"Number of rows:\", len(data))\n",
        "print(\"Column names:\", data.columns.tolist())\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(data.head())\n",
        "\n",
        "\n",
        "# what is input?:Ans => numeric features like Length, Diameter, Height, etc.\n",
        "# what is output?Ans : Rings (which represents abalone age)\n",
        "# why output is numeric?Ans : because age is a continuous value (regression problem)\n",
        "\n",
        "# A2. Convert target\n",
        "#  create target y = Rings + 1.5\n",
        "data['Target'] = data['Rings'] + 1.5\n",
        "\n",
        "# A3. Choose exactly 3 numeric features\n",
        "#  select exactly 3 numeric features\n",
        "features = ['Length', 'Diameter', 'Height']# i choose lenght,dia,height\n",
        "X = data[features].values\n",
        "y = data['Target'].values.reshape(-1, 1)\n",
        "\n",
        "# Justification for features that i selected:\n",
        "# Feature 1 (Length): indicates size; larger abalone are usually older.\n",
        "# Feature 2 (Diameter): correlates with shell width and maturity.\n",
        "# Feature 3 (Height): adds volume dimension; complements size-related prediction.\n",
        "\n",
        "# A4. Train-test split (80/20)\n",
        "split_index = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "print(\"\\nTrain shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Test shape:\", X_test.shape, y_test.shape)\n",
        "\n",
        "# A5. Normalize inputs (using training mean and std only)\n",
        "mean = X_train.mean(axis=0)\n",
        "std = X_train.std(axis=0)\n",
        "X_train_norm = (X_train - mean) / std\n",
        "X_test_norm = (X_test - mean) / std\n",
        "\n",
        "# why normalization is needed for learning:\n",
        "# Ensures all features contribute equally and gradient descent converges efficiently.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evkFGZTDQVVI",
        "outputId": "1c15e83d-062f-4954-a8e9-b76bd85f2c28"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 4177\n",
            "Column names: ['Sex', 'Length', 'Diameter', 'Height', 'WholeWeight', 'ShuckedWeight', 'VisceraWeight', 'ShellWeight', 'Rings']\n",
            "\n",
            "First 5 rows:\n",
            "  Sex  Length  Diameter  Height  WholeWeight  ShuckedWeight  VisceraWeight  \\\n",
            "0   M   0.455     0.365   0.095       0.5140         0.2245         0.1010   \n",
            "1   M   0.350     0.265   0.090       0.2255         0.0995         0.0485   \n",
            "2   F   0.530     0.420   0.135       0.6770         0.2565         0.1415   \n",
            "3   M   0.440     0.365   0.125       0.5160         0.2155         0.1140   \n",
            "4   I   0.330     0.255   0.080       0.2050         0.0895         0.0395   \n",
            "\n",
            "   ShellWeight  Rings  \n",
            "0        0.150     15  \n",
            "1        0.070      7  \n",
            "2        0.210      9  \n",
            "3        0.155     10  \n",
            "4        0.055      7  \n",
            "\n",
            "Train shape: (3341, 3) (3341, 1)\n",
            "Test shape: (836, 3) (836, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Part B: Define the Model\n",
        "\n",
        "def forward(X, w, b):\n",
        "    \"\"\"\n",
        "    Computes y_hat = Xw + b\n",
        "    \"\"\"\n",
        "    y_hat = np.dot(X, w) + b\n",
        "    # Print shapes once\n",
        "    print(f\"Shapes -> X: {X.shape}, w: {w.shape}, b: {b.shape}, y_hat: {y_hat.shape}\")\n",
        "    return y_hat\n",
        "\n",
        "# parameters are: weights (w1, w2, w3) and bias (b)\n",
        "# number of parameters: 4 (3 weights + 1 bias)"
      ],
      "metadata": {
        "id": "CCKs4LttR4mZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Part C: Define Loss (MSE)\n",
        "\n",
        "def mse(y, y_hat):\n",
        "    \"\"\"\n",
        "    Mean Squared Error\n",
        "    \"\"\"\n",
        "    loss = np.mean((y - y_hat) ** 2)\n",
        "    return loss\n",
        "\n",
        "# why square? ans: Because it emphasizes larger errors, ensures non-negative loss\n",
        "# what mistakes are expensive? ans: large deviations between prediction and target"
      ],
      "metadata": {
        "id": "ctpLXdkUSYoA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Part D: Gradients and Learning Rule\n",
        "\n",
        "# what “gradient” means in words:\n",
        "# → Direction and rate of steepest increase in loss function.\n",
        "# why subtracting the gradient reduces loss:\n",
        "# → It moves parameters in the direction where loss decreases.\n",
        "\n",
        "def grad_w(X, y, y_hat):\n",
        "    \"\"\"\n",
        "    Gradient of loss with respect to weights\n",
        "    \"\"\"\n",
        "    N = len(y)\n",
        "    dW = (-2 / N) * np.dot(X.T, (y - y_hat))\n",
        "    return dW\n",
        "\n",
        "def grad_b(y, y_hat):\n",
        "    \"\"\"\n",
        "    Gradient of loss with respect to bias\n",
        "    \"\"\"\n",
        "    N = len(y)\n",
        "    db = (-2 / N) * np.sum(y - y_hat)\n",
        "    return db\n",
        "\n",
        "# meaning of large gradient: model making large errors, parameters need significant adjustment\n",
        "# effect of too-large learning rate: updates overshoot optimal values, causing divergence"
      ],
      "metadata": {
        "id": "R7DsssoSSliD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Part E: Training Loop\n",
        "\n",
        "# Initialize weights and bias\n",
        "np.random.seed(42)\n",
        "w = np.random.randn(X_train_norm.shape[1], 1) * 0.01\n",
        "b = np.zeros((1,))\n",
        "\n",
        "# Hyperparameters\n",
        "lr = 0.01\n",
        "epochs = 200\n",
        "\n",
        "# Initial expectation:\n",
        "# The loss should gradually decrease as the model learns patterns.\n",
        "for epoch in range(epochs):\n",
        "    # 1) Forward pass\n",
        "    y_hat = forward(X_train_norm, w, b)\n",
        "\n",
        "    # 2) Compute loss\n",
        "    loss = mse(y_train, y_hat)\n",
        "\n",
        "    # 3) Compute gradients\n",
        "    dW = grad_w(X_train_norm, y_train, y_hat)\n",
        "    db = grad_b(y_train, y_hat)\n",
        "\n",
        "    # 4) Update parameters\n",
        "    w -= lr * dW\n",
        "    b -= lr * db\n",
        "\n",
        "    # Print progress\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
        "\n",
        "# Revised expectation after training:\n",
        "# The loss decreases steadily and stabilizes, showing convergence."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L02nnRtTyai",
        "outputId": "f1e0b899-b7c4-43aa-c1f3-4d70502e27d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Epoch 0: Loss = 144.2634\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Epoch 20: Loss = 67.0221\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Epoch 40: Loss = 33.7606\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Epoch 60: Loss = 19.0564\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Epoch 80: Loss = 12.5142\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Epoch 100: Loss = 9.5985\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Epoch 120: Loss = 8.2979\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Epoch 140: Loss = 7.7173\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Epoch 160: Loss = 7.4575\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Epoch 180: Loss = 7.3407\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n",
            "Shapes -> X: (3341, 3), w: (3, 1), b: (1,), y_hat: (3341, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Part F: Evaluation\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_test = forward(X_test_norm, w, b)\n",
        "\n",
        "# Compute Test MSE and MAE\n",
        "test_mse = mse(y_test, y_pred_test)\n",
        "test_mae = np.mean(np.abs(y_test - y_pred_test))\n",
        "\n",
        "print(\"\\nTest MSE:\", test_mse)\n",
        "print(\"Test MAE:\", test_mae)\n",
        "\n",
        "# Display 5 sample predictions\n",
        "print(\"\\nSample Predictions (True | Predicted | Abs Error):\")\n",
        "for i in range(5):\n",
        "    true_val = y_test[i][0]\n",
        "    pred_val = y_pred_test[i][0]\n",
        "    abs_err = abs(true_val - pred_val)\n",
        "    print(f\"{i+1}) {true_val:.2f} | {pred_val:.2f} | {abs_err:.2f}\")\n",
        "\n",
        "# which cases seem systematically wrong: usually very young or very old abalones\n",
        "# observed bias: model may underpredict extreme ages (bias toward mean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwY7Zt5rUgb-",
        "outputId": "ee2a5d77-4c10-4375-8645-d01d92b12ffb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes -> X: (836, 3), w: (3, 1), b: (1,), y_hat: (836, 1)\n",
            "\n",
            "Test MSE: 4.893151796903823\n",
            "Test MAE: 1.6583424459987677\n",
            "\n",
            "Sample Predictions (True | Predicted | Abs Error):\n",
            "1) 13.50 | 10.99 | 2.51\n",
            "2) 15.50 | 9.55 | 5.95\n",
            "3) 14.50 | 10.19 | 4.31\n",
            "4) 14.50 | 11.11 | 3.39\n",
            "5) 13.50 | 11.05 | 2.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#\n",
        "#  Deliverables Summary — UCS761 Deep Learning Lab 5\n",
        "#\n",
        "\n",
        "## Included in this Notebook\n",
        "1. **Dataset Loading and Feature Choice Justification**\n",
        "   - Loaded Abalone dataset (local `abalone.data` file)\n",
        "   - Chose 3 numeric features: `Length`, `Diameter`, `Height`\n",
        "   - Justification: All three correlate strongly with abalone size and age\n",
        "\n",
        "2. **Linear Model Forward Pass**\n",
        "   - Implemented function `forward(X, w, b)` to compute `y_hat = Xw + b`\n",
        "   - Verified matrix shapes of X, w, b, and y_hat\n",
        "\n",
        "3. **Loss Functions (MSE & MAE)**\n",
        "   - Implemented `mse(y, y_hat)`  \n",
        "   - Computed test MSE and MAE for evaluation\n",
        "\n",
        "4. **Gradients and Update Rule**\n",
        "   - Derived and implemented `grad_w()` and `grad_b()` for parameter updates\n",
        "   - Explained gradient meaning and why subtracting reduces loss\n",
        "\n",
        "5. **Training Loop**\n",
        "   - Implemented full training loop:\n",
        "     - Forward pass → Loss → Gradients → Parameter Update\n",
        "   - Observed steadily decreasing loss over epochs\n",
        "\n",
        "6. **Evaluation and Predictions**\n",
        "   - Printed Test MSE & MAE\n",
        "   - Displayed 5 sample predictions with:\n",
        "     - True value\n",
        "     - Predicted value\n",
        "     - Absolute error\n",
        "\n",
        "7. **Reflective Comments**\n",
        "   - Initially expected loss to drop fast, but it decreased gradually\n",
        "   - Observed bias toward mean ages (underpredicts very young/old)\n",
        "   - Learned how gradient descent updates weights to minimize error\n",
        "\n",
        "---\n",
        "\n",
        "##  What I Did NOT Use\n",
        "-  No **scikit-learn Linear Regression**\n",
        "-  No **torch** or **keras**\n",
        "-  No **hidden layers**\n",
        "-  Verified all formulas manually and ensured correct array shapes\n",
        "\n",
        "---\n",
        "\n",
        "###  Reflection\n",
        "> “I learned how a model updates numbers (weights & bias) to reduce error.”"
      ],
      "metadata": {
        "id": "_RWo1cDFVO5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Reflective Comments\n",
        "\n",
        "- **Initial expectation:** I expected the loss to decrease gradually, as gradient descent finds the optimal parameters.\n",
        "- **Observed result:** The loss started around 144 and converged near 7, showing successful learning.\n",
        "- **Learning behavior:** The loss reduced smoothly without divergence — normalization and learning-rate choice were appropriate.\n",
        "- **Systematic errors:** The model underpredicts for very old abalones (higher Rings values), indicating slight bias toward the mean.\n",
        "- **Takeaway:** I understood how each iteration updates weights and bias to reduce error. This builds intuition for how deeper networks learn."
      ],
      "metadata": {
        "id": "AkOi9QNxWPlN"
      }
    }
  ]
}